# Framework for "Segment-based classification of three-dimensional laser range data"

This document discusses the setup and first steps for setting up your system.

## Dependencies

The code is tested under Ubuntu 12.04 and Ubuntu 14.04. It depends on the following packages:

* cmake (>= 2.6)
* libeigen (>= 3.0) 
* libboost (>= 1.46)
* Qt4 (>= 4.8)

The following command should install all needed dependencies:

Ubuntu 12.04/14.04:
  $ sudo apt-get install build-essential cmake libeigen3-dev libboost-all-dev libqt4-dev libqt4-designer
  
We tested this on Ubuntu 12.04.05 LTS and Ubuntu 14.04.02 LTS, but it should also work for versions in between.

## Setup of your project and compilation

In the following, we assume that your are already in the directory with your working copy of the directory.

1. Execute the setup script "setup.sh", which downloads the necessary data and creates the "build" directory:
  $ sh setup.sh
  
  The script performs the fowllowing steps:
  (a) Download the data from http://www.iai.uni-bonn.de/~behley/data/sdi2015-data.zip
  (b) Extract the data in the "data" directory and removes the zip.
  (c) Create a "build" directory, which is used to store the object files, etc.
  (d) Calls "cmake .." from the "build" directory to generate the Makefile
  (e) Calls "make" inside the build directory to compile the framework.
  (f) Calls "./runtests" in the base directory of the framework.

Now there should be a "data" folder containing the training and test data for this project and a "build" directory
in your working copy. You should not commit these folders to the repository, since these can be recreated with the 
setup provided script. You only have to commit your changes to the source files, your final models and your final 
configuration. All executables, data, and temporary build files are only needed locally on your machine.
  
2. Execute "./runtests", which should run the provided tests and also should fail for all test cases:
  $ ./runtests
  
For grading purposes, we will use the same tests to check if you provided a working solution of the subtasks. 

## Notes on the provided framework

Your code and implementation should go into the provided classes in the sub-folder "project". Your final
solution should complete the methods with missing code. For all subtasks, we provide "test cases", which 
should pass if your code solves the subtask. The test cases are located in "tests".

The sub-directory "config" contains some example configurations for training of the models and the inference
on the test data using these models. You are free to copy, to extent and modify this examples as you need more
parameters or a different directory structure.
The sub-directory "model" contains the learned models and will contain your solution.
The sub-directory "doc" should contain your document explaining your solution with some own contributions.

For your final submission, you have to provide a implementation of the subtasks and may additionally provide 
your configifugartion and model in the "config" and "model" directory with name "final" respectively, i.e., 
      "config/final_train.xml", "config/final_test.xml" and "model/final/".
Don't forget to add a PDF document under "doc", which explains your own contribution. See also the author kit
provided in Ecampus with an example document, which could be the foundation for your own write-up.
      
We will only consider solutions following this naming convention for grading. You _DON'T_ need to upload your
predictions, since we can reproduce them with the configuration & model files!

After compiling the code, you should find five executables in the root directory:

1. "train-dictionary" is used to sample descriptors from the provided training data and starts the
  k-means clustering to generate a vocabulary for your bag-of-words. You have to provide a config file, 
  see for an example "example_train.xml"
2. "train-classifier" is then used to learn the classifier with the provided vocabulary, which is specified
  in the configuration file. The provided implementation computes the bag-of-words and passes the feature vectors
  and the labels to the "train" method of the classifier, which is then stored in a separate file.
3. "classify-scans" takes the learned models and applies these to a provided set of laser range scans, where now
  the laser scans are first segmented. A label is generated for each segment and the segments and  predictions are 
  stored in a given directory. See "example_test.xml" for an example specification.
4. "pviz" is a Qt4 application, which can be used to visualize the point clouds with ground truth annotation and
  the predictions generated by your approach. First you have to load the "laser scans" from a directory and then 
  your predictions.
5. "score" calculates the average precision for your prediction in respect to a given ground truth. The program
  is called with first the ground truth directory and then the directory containing the predictions.

## Troubleshooting

Problem: With apt-get many packages needs to be removed! Especially, libqt4-dev causes the problem.

Resolution: In Ubuntu Gnome 14.04.02 LTS, we experienced some problems with the dependencies, which caused the 
removal of the GNOME display manager, which you want to avoid. You should check that removed packages are not 
breaking your system. Generally, there should be no packages removed at all! If you encounter that some 
dependencies need the removal of viable parts: Try aptitude and choose another option for fixing dependency 
clashes. For us, there was a solution, which does not install/update some components of mesa-gl, but this is no
problem

---

That's all. If you have questions regarding the framework or problems with the setup, don't hesitate to contact me
via behley@iai.uni-bonn.de.
